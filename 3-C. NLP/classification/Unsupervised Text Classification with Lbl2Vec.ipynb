{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de\n",
    "# https://github.com/sebischair/Lbl2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lbl2vec import Lbl2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "# lets check our keywords\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# add data set type column\n",
    "newsgroup_train['data_set_type'] = 'train'\n",
    "newsgroup_test['data_set_type'] = 'test'\n",
    "\n",
    "# concat train and test data\n",
    "newsgroup_full_corpus = pd.concat([newsgroup_train,newsgroup_test]).reset_index(drop=True)\n",
    "\n",
    "# reduce dataset to only articles that belong to classes where we defined our keywords\n",
    "newsgroup_full_corpus = newsgroup_full_corpus[newsgroup_full_corpus['class_index'].isin(list(labels['class_index']))]\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "newsgroup_full_corpus['tagged_docs'] = newsgroup_full_corpus.apply(lambda row: TaggedDocument(tokenize(row['article']), [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "newsgroup_full_corpus['doc_key'] = newsgroup_full_corpus.index.astype(str)\n",
    "\n",
    "# add class_name column\n",
    "newsgroup_full_corpus = newsgroup_full_corpus.merge(labels, left_on='class_index', right_on='class_index', how='left')\n",
    "\n",
    "print(newsgroup_full_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Training Lbl2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=100, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Classification of Text Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = Lbl2Vec_model.predict_model_docs()\n",
    "\n",
    "# merge DataFrames to compare the predicted and true category labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type'] == 'train'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict similarity scores of new test documents (they were not used during Lbl2Vec training)\n",
    "new_docs_lbl_similarities = Lbl2Vec_model.predict_new_docs(tagged_docs=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type']=='test'])\n",
    "\n",
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_test = new_docs_lbl_similarities.merge(newsgroup_full_corpus[newsgroup_full_corpus['data_set_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_test = evaluation_test['class_name']\n",
    "y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_test, y_pred_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
